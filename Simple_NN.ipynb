{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01d34a25-7847-4a39-9c59-bc4ef2f3eb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00a353a3-221e-47fe-85e3-23745d508ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10c3c411-f60b-47bd-ae3a-a4340d8e8838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the training set of bigrams (x,y)\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words:\n",
    "    chars = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chars, chars[1:]):\n",
    "        idx1 = stoi[ch1]\n",
    "        idx2 = stoi[ch2]\n",
    "        xs.append(idx1)\n",
    "        ys.append(idx2)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9f846f2b-2ad6-40a0-86fc-08ef302daa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0d7a2e63-3fc7-4f80-beb2-c07be7240801",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27,27), generator = g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8bfe9677-536e-4676-a1d9-cc351eb5d680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228146, 27])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc = F.one_hot(xs, num_classes=27).float()\n",
    "xenc.shape\n",
    "#plt.imshow(xenc[:20])\n",
    "logits = (xenc @ W)\n",
    "counts = logits.exp()\n",
    "prob = counts / counts.sum(1, keepdims = True)\n",
    "prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3f1d8287-4022-4390-8c76-ebdd3dd8bbd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------\n",
      "bigram example 1: .e (indices 0, 5)\n",
      "input to the neural net :  0\n",
      "output prob from neural net :  tensor([0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n",
      "        0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n",
      "        0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459])\n",
      "label (actual next character): 5\n",
      "probability assigned by the net to the correct character: 0.012286253273487091\n",
      "log likelihood: -4.3992743492126465\n",
      "negative log likelihood: 4.3992743492126465\n",
      "--------------------------------------------------------------------------\n",
      "bigram example 2: em (indices 5, 13)\n",
      "input to the neural net :  5\n",
      "output prob from neural net :  tensor([0.0290, 0.0796, 0.0248, 0.0521, 0.1989, 0.0289, 0.0094, 0.0335, 0.0097,\n",
      "        0.0301, 0.0702, 0.0228, 0.0115, 0.0181, 0.0108, 0.0315, 0.0291, 0.0045,\n",
      "        0.0916, 0.0215, 0.0486, 0.0300, 0.0501, 0.0027, 0.0118, 0.0022, 0.0472])\n",
      "label (actual next character): 13\n",
      "probability assigned by the net to the correct character: 0.018050702288746834\n",
      "log likelihood: -4.014570713043213\n",
      "negative log likelihood: 4.014570713043213\n",
      "--------------------------------------------------------------------------\n",
      "bigram example 3: mm (indices 13, 13)\n",
      "input to the neural net :  13\n",
      "output prob from neural net :  tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
      "        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
      "        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\n",
      "label (actual next character): 13\n",
      "probability assigned by the net to the correct character: 0.026691533625125885\n",
      "log likelihood: -3.623408794403076\n",
      "negative log likelihood: 3.623408794403076\n",
      "--------------------------------------------------------------------------\n",
      "bigram example 4: ma (indices 13, 1)\n",
      "input to the neural net :  13\n",
      "output prob from neural net :  tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
      "        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
      "        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\n",
      "label (actual next character): 1\n",
      "probability assigned by the net to the correct character: 0.07367684692144394\n",
      "log likelihood: -2.6080667972564697\n",
      "negative log likelihood: 2.6080667972564697\n",
      "--------------------------------------------------------------------------\n",
      "bigram example 5: a. (indices 1, 0)\n",
      "input to the neural net :  1\n",
      "output prob from neural net :  tensor([0.0150, 0.0086, 0.0396, 0.0100, 0.0606, 0.0308, 0.1084, 0.0131, 0.0125,\n",
      "        0.0048, 0.1024, 0.0086, 0.0988, 0.0112, 0.0232, 0.0207, 0.0408, 0.0078,\n",
      "        0.0899, 0.0531, 0.0463, 0.0309, 0.0051, 0.0329, 0.0654, 0.0503, 0.0091])\n",
      "label (actual next character): 0\n",
      "probability assigned by the net to the correct character: 0.0149775305762887\n",
      "log likelihood: -4.201204299926758\n",
      "negative log likelihood: 4.201204299926758\n",
      "================================================================================\n",
      "average negative log likelihood i.e., loss :  3.7693049907684326\n"
     ]
    }
   ],
   "source": [
    "nlls = torch.zeros(5)\n",
    "for i in range(5):\n",
    "    x = xs[i].item()\n",
    "    y = ys[i].item()\n",
    "    print('--------------------------------------------------------------------------')\n",
    "    print(f'bigram example {i+1}: {itos[x]}{itos[y]} (indices {x}, {y})')\n",
    "    print('input to the neural net : ', x)\n",
    "    print('output prob from neural net : ', prob[i])\n",
    "    print('label (actual next character):', y)\n",
    "    p = prob[i, y]\n",
    "    print('probability assigned by the net to the correct character:', p.item())\n",
    "    logp = torch.log(p)\n",
    "    print('log likelihood:', logp.item())\n",
    "    nll = -logp\n",
    "    print('negative log likelihood:', nll.item())\n",
    "    nlls[i] = nll\n",
    "\n",
    "print(\"================================================================================\")\n",
    "print('average negative log likelihood i.e., loss : ', nlls.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "13d99d51-f1a7-424e-8e57-becbbd3487da",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27,27), generator = g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "326524e9-c620-4c16-97d8-e79bb201de13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#foreward pass\n",
    "xenc = F.one_hot(xs, num_classes=27).float()\n",
    "logits = (xenc @ W)\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdims = True)\n",
    "loss = -probs[torch.arange(5), ys[:5]].log().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d309c063-2960-4cbb-9fd7-af2ddc4956e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward pass\n",
    "W.grad = None #zero the gradients\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "af7cce03-5f98-4375-be45-415d8d04e709",
   "metadata": {},
   "outputs": [],
   "source": [
    "W.data += -0.1 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "80cdad37-3f8e-4227-8855-68f34f5122dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7291626930236816\n"
     ]
    }
   ],
   "source": [
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dfd5eb-3653-4baa-9c71-e159773b56d3",
   "metadata": {},
   "source": [
    "**Complete flow :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ed4375d2-0652-4caf-a5d0-d1e343c1297e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  228146\n"
     ]
    }
   ],
   "source": [
    "#creating the dataset\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "    chars = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chars, chars[1:]):\n",
    "        idx1 = stoi[ch1]\n",
    "        idx2 = stoi[ch2]\n",
    "        xs.append(idx1)\n",
    "        ys.append(idx2)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print('number of examples: ', num)\n",
    "\n",
    "# initialize the simple neural net\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27,27), generator = g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a65a1c50-3dd7-471a-a9d1-7587087f1754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4813597202301025\n",
      "2.481353998184204\n",
      "2.4813482761383057\n",
      "2.481342315673828\n",
      "2.4813365936279297\n",
      "2.4813311100006104\n",
      "2.481325387954712\n",
      "2.4813199043273926\n",
      "2.481314182281494\n",
      "2.481308698654175\n",
      "2.4813032150268555\n",
      "2.481297492980957\n",
      "2.481292486190796\n",
      "2.4812870025634766\n",
      "2.4812819957733154\n",
      "2.481276750564575\n",
      "2.481271266937256\n",
      "2.4812662601470947\n",
      "2.4812610149383545\n",
      "2.4812560081481934\n",
      "2.4812510013580322\n",
      "2.481245994567871\n",
      "2.48124098777771\n",
      "2.481236219406128\n",
      "2.4812307357788086\n",
      "2.4812262058258057\n",
      "2.4812211990356445\n",
      "2.4812166690826416\n",
      "2.4812119007110596\n",
      "2.4812071323394775\n",
      "2.4812026023864746\n",
      "2.481198310852051\n",
      "2.4811933040618896\n",
      "2.4811887741088867\n",
      "2.481184244155884\n",
      "2.481179714202881\n",
      "2.481175184249878\n",
      "2.481170892715454\n",
      "2.4811666011810303\n",
      "2.4811618328094482\n",
      "2.4811577796936035\n",
      "2.481153726577759\n",
      "2.481149435043335\n",
      "2.481145143508911\n",
      "2.4811408519744873\n",
      "2.4811365604400635\n",
      "2.481132745742798\n",
      "2.481128215789795\n",
      "2.48112416267395\n",
      "2.4811203479766846\n",
      "2.48111629486084\n",
      "2.481112241744995\n",
      "2.4811084270477295\n",
      "2.4811043739318848\n",
      "2.48110032081604\n",
      "2.4810965061187744\n",
      "2.481092691421509\n",
      "2.481088638305664\n",
      "2.4810853004455566\n",
      "2.481081247329712\n",
      "2.4810774326324463\n",
      "2.4810736179351807\n",
      "2.481070041656494\n",
      "2.4810664653778076\n",
      "2.481062889099121\n",
      "2.4810593128204346\n",
      "2.481055736541748\n",
      "2.4810519218444824\n",
      "2.481048583984375\n",
      "2.4810447692871094\n",
      "2.481041431427002\n",
      "2.4810380935668945\n",
      "2.481034755706787\n",
      "2.4810311794281006\n",
      "2.481027841567993\n",
      "2.4810245037078857\n",
      "2.481020927429199\n",
      "2.481017589569092\n",
      "2.4810144901275635\n",
      "2.481011152267456\n",
      "2.4810080528259277\n",
      "2.4810047149658203\n",
      "2.481001377105713\n",
      "2.4809982776641846\n",
      "2.4809951782226562\n",
      "2.480992078781128\n",
      "2.4809892177581787\n",
      "2.4809861183166504\n",
      "2.480982542037964\n",
      "2.4809799194335938\n",
      "2.4809768199920654\n",
      "2.4809741973876953\n",
      "2.480970621109009\n",
      "2.4809679985046387\n",
      "2.4809648990631104\n",
      "2.480962038040161\n",
      "2.480959415435791\n",
      "2.4809563159942627\n",
      "2.4809532165527344\n",
      "2.480950355529785\n"
     ]
    }
   ],
   "source": [
    "for k in range(100):\n",
    "    #foreward pass\n",
    "    xenc = F.one_hot(xs, num_classes=27).float()\n",
    "    logits = (xenc @ W)\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims = True)\n",
    "    loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean() # regularizing to get non zero probs\n",
    "    print(loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None #zero the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    #update\n",
    "    W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "88d073db-9b30-458e-a407-bec91d26a8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cexze\n",
      "momasurailezityha\n",
      "konimittain\n",
      "llayn\n",
      "ka\n",
      "da\n",
      "staiyaubrtthrigotai\n",
      "moliellavo\n",
      "ke\n",
      "teda\n",
      "ka\n",
      "emimmsade\n",
      "enkaviyny\n",
      "ftlsp\n",
      "hinivenvtahlasu\n",
      "dsor\n",
      "br\n",
      "jol\n",
      "pen\n",
      "aisan\n"
     ]
    }
   ],
   "source": [
    "# sample from Neural Net Model\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(20):\n",
    "    out = []\n",
    "    ix = 0\n",
    "    while True:\n",
    "        # p = P[ix] --> before\n",
    "        xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "        logits = (xenc @ W)\n",
    "        counts = logits.exp()\n",
    "        p = counts / counts.sum(1, keepdims = True)\n",
    "        \n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        if ix == 0:\n",
    "            break\n",
    "        out.append(itos[ix])\n",
    "    print(''.join(out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
